Carregando as bibliotecas do projeto

```{r}
# Carregando as bibliotecas
library(tidyverse)
library(pROC)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(yardstick)
library(rsample)
library(dplyr)
library(glmnet)
library(rpart)
library(partykit)
library(vip)
set.seed(907)
```

Importando os dados

```{r}
#importando os Dados
dados <- read.csv("dados_para_o_R.csv") # nolint
head(dados)
```

Criando uma tabela para guardar as métricas dos modelos gerados
```{r}
# criando tibble para comparar modelos:
models_tibble <- tibble(modelo = c("glm", "ridge", "LASSO", 
                                   "Arv.Decisão", "Rd.Forest"),
                        Accuracy = NA * length(modelo), Precision = NA * length(modelo), ROC_Curve = NA * length(modelo),
                        FP = NA * length(modelo), FN = NA * length(modelo))
models_tibble
```
Criando uma lista e uma função para as curvas ROC do modelo:
```{r}

# Função para calcular e armazenar as curvas ROC
calcular_roc <- function(truth, prob, modelo_nome) {
  roc_obj <- roc(truth, prob)
  data.frame(
    tpr = roc_obj$sensitivities,   # True positive rate (sensibilidade)
    fpr = 1 - roc_obj$specificities, # False positive rate (1 - especificidade)
    modelo = modelo_nome
  )
}

# Lista para armazenar todas as curvas ROC
roc_list <- list()
```

Criando uma função para adicionar os resultados das funções na tabela:
```{r}
adiciona_tabela <- function(modelo_nome, resultados, models_tibble) {
  
  # Reordenando os níveis da variável 'truth'
  #resultados$truth <- factor(resultados$truth, levels = c(1, 0))
  # Reordenando os níveis da variável 'estimate' para corresponder a 'truth'
  #resultados$estimate <- factor(resultados$estimate, levels = c(1, 0))
  
  # Calculando a matriz de confusão para FP e FN
  conf_matrix <- table(resultados$truth, resultados$estimate)
  
  # Extraindo falsos positivos (0 predito como 1) e falsos negativos (1 predito como 0)
  false_positives <- conf_matrix["0", "1"]
  false_negatives <- conf_matrix["1", "0"]
  
  # Calculando o número total de previsões
  total_previsoes <- sum(conf_matrix)
  
  # Calculando as porcentagens de FP e FN
  fp_percent <- (false_positives / total_previsoes) * 100
  fn_percent <- (false_negatives / total_previsoes) * 100
  
  # Calculando as métricas existentes
  accuracy_result <- yardstick::accuracy(resultados, truth = truth, estimate = estimate)
  precision_result <- yardstick::precision(resultados, truth = truth, estimate = estimate)
  roc_auc_result <- yardstick::roc_auc(resultados, truth = truth, .pred_1)
  
  # Adicionando novas métricas (FP e FN em %) à tabela models_tibble
  models_tibble$Accuracy[models_tibble$modelo == modelo_nome] <- accuracy_result$.estimate
  models_tibble$Precision[models_tibble$modelo == modelo_nome] <- precision_result$.estimate
  models_tibble$ROC_Curve[models_tibble$modelo == modelo_nome] <- roc_auc_result$.estimate
  
 
  # Atualizando os valores de FP e FN (em %)
  models_tibble$FP[models_tibble$modelo == modelo_nome] <- fp_percent
  models_tibble$FN[models_tibble$modelo == modelo_nome] <- fn_percent
  
  # Retornando a tabela atualizada
  return(models_tibble)
}

```

Criando nosso "Sarrafo", nosso limiar de decisão de quando a probabilidade de classe definirá ou não a classe predita
```{r}
#sarrafo universal para classificação de todos os modelos
sarrafo <- 0.7
```

Separando os dados em treino e teste e ordenando as variaveis resposta
1 -> empresa continua operando
0 -> empresa parou de operar
```{r}

# Separando os dados em treino e teste
split <- initial_split(dados, prop = 0.7)
treinamento <- training(split)
teste <- testing(split)

treinamento$operates_within_2_years <- factor(treinamento$operates_within_2_years, levels = c(0, 1))
teste$operates_within_2_years <- factor(teste$operates_within_2_years, levels = c(0, 1))
```

A partir daqui geraremos os modelos com base nos dados

O primeiro modelo será a regressão logística:
```{r}

# i) regressão logistica ----------------------------------------------------------

# Preparando os dados
x_train <- model.matrix(operates_within_2_years ~ ., data = treinamento)[, -1]
y_train <- treinamento$operates_within_2_years

x_test <- model.matrix(operates_within_2_years ~ ., data = teste)[, -1]
y_test <- teste$operates_within_2_years

# Definindo a sequência de lambdas (incluindo valores muito pequenos)
lambda_seq <- 10^seq(-6, 0, length = 100)

# Ajustando o modelo com cv.glmnet()
cv_logistic <- cv.glmnet(x_train, y_train, alpha = 0, family = "binomial", lambda = lambda_seq)

# Obtendo o melhor lambda
best_lambda <- cv_logistic$lambda.min
cat("Melhor lambda:", best_lambda, "\n")

# Fazendo previsões no conjunto de teste
y_pred_prob <- predict(cv_logistic, s = best_lambda, newx = x_test, type = "response")
y_pred_prob <- as.numeric(y_pred_prob)

# Obtendo as previsões de classe com base no threshold
y_pred <- as.factor(ifelse(y_pred_prob > sarrafo, 1, 0))

# Valores reais das classes no conjunto de teste
y_real <- as.factor(y_test)

# Criando um tibble com os resultados
resultados <- tibble(
  truth = y_real,
  estimate = y_pred,
  .pred_1 = y_pred_prob
)

# Atualizando a tabela models_tibble
models_tibble <- adiciona_tabela("glm", resultados, models_tibble)

roc_logistic <- calcular_roc(resultados$truth, resultados$.pred_1, "GLM")
roc_list[[1]] <- roc_logistic
```
```{r}
vip(cv_logistic)
```

Em seguida faremos a regressão ridge:
```{r}
# ii) regressão ridge ----------------------------------------------------------

# Convertendo as variáveis preditoras e resposta para matriz
x_train <- model.matrix(operates_within_2_years ~ ., data = treinamento)[, -1]
y_train <- treinamento$operates_within_2_years

x_test <- model.matrix(operates_within_2_years ~ ., data = teste)[, -1]
y_test <- teste$operates_within_2_years

# Ajustando o modelo Ridge com glmnet
ridge_model <- glmnet(x_train, y_train, alpha = 0, family = "binomial")

# Validação cruzada para encontrar o melhor lambda
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, family = "binomial")

# Obtendo o melhor lambda
best_lambda <- cv_ridge$lambda.min
cat("Melhor lambda:", best_lambda, "\n")

# Fazendo previsões no conjunto de teste
y_pred_prob <- predict(ridge_model, s = best_lambda, newx = x_test, type = "response")

# Convertendo as probabilidades para vetor numérico
y_pred_prob <- as.numeric(y_pred_prob)

# Obtendo as previsões de classe com base no threshold
y_pred <- as.factor(ifelse(y_pred_prob > sarrafo, 1, 0))

# Valores reais das classes no conjunto de teste
y_real <- as.factor(teste$operates_within_2_years)

# Criando um tibble com os resultados
resultados <- tibble(
  truth = y_real,
  estimate = y_pred,
  .pred_1 = y_pred_prob
)

models_tibble <- adiciona_tabela("ridge", resultados, models_tibble)

roc_ridge <- calcular_roc(resultados$truth, resultados$.pred_1, "Ridge")
roc_list[[2]] <- roc_ridge
```
```{r}
vip(ridge_model)
```

regressão LASSO:
```{r}
# iii) regressão LASSO ---------------------------------------------------------


# ajustando o modelo
lasso_model <- glmnet(x_train, y_train, alpha = 1, family = "binomial") # alpha =1 para Lasso

#enconrando o melhor lambda
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
best_lambda <- cv_lasso$lambda.min
cat("Melhor lambda:", best_lambda, "\n")

# Fazendo previsões no conjunto de teste
y_pred_prob <- predict(lasso_model, s = best_lambda, newx = x_test, type = "response")

# Convertendo as probabilidades para vetor numérico
y_pred_prob <- as.numeric(y_pred_prob)

# Obtendo as previsões de classe com base no threshold
y_pred <- as.factor(ifelse(y_pred_prob > sarrafo, 1, 0))

# Valores reais das classes no conjunto de teste
y_real <- as.factor(teste$operates_within_2_years)

# Criando um tibble com os resultados
resultados <- tibble(
  truth = y_real,
  estimate = y_pred,
  .pred_1 = y_pred_prob
)

models_tibble <- adiciona_tabela("LASSO", resultados, models_tibble)

roc_lasso <- calcular_roc(resultados$truth, resultados$.pred_1, "LASSO")
roc_list[[3]] <- roc_lasso
```
```{r}
vip(lasso_model)
```

Arvore de Decisão:
```{r}
# iv) árvore de decisão ------------------------------------------
library(rpart.plot)
tree <- rpart(operates_within_2_years ~ ., data = treinamento, method = "class")

# Visualizando a árvore e o gráfico de CP
rpart.plot(tree, roundint = FALSE)
plotcp(tree)

# Encontrando o CP ótimo e podando a árvore
cp_ot <- tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"]
tree <- prune(tree, cp = cp_ot)
rpart.plot(tree, roundint = FALSE)

# Fazendo previsões no conjunto de teste
y_pred_prob <- predict(tree, newdata = teste, type = "prob")[, 2]  # Probabilidade da classe 1

# Aplicando o threshold 'sarrafo' para obter as previsões de classe
y_pred <- as.factor(ifelse(y_pred_prob > sarrafo, 1, 0))

# Convertendo os valores reais em fator
y_real <- as.factor(teste$operates_within_2_years)

# Criando o tibble de resultados
resultados <- tibble(
  truth = y_real,       # Valores reais
  estimate = y_pred,    # Valores previstos
  .pred_1 = y_pred_prob # Probabilidades previstas para a classe 1
)

# Calculando a AUC da curva ROC
models_tibble <- adiciona_tabela("Arv.Decisão", resultados, models_tibble)

roc_arvore <- calcular_roc(resultados$truth, resultados$.pred_1, "Arvore de Decisão")
roc_list[[4]] <- roc_arvore
```
```{r}
vip(tree)
```
Floresta Aleatória:
```{r}
# v) floresta aleatória ------------------------------------------
library(ranger)

# Criando folds de validação cruzada a partir do conjunto de treinamento
folds <- vfold_cv(treinamento, v = 5, strata = operates_within_2_years)

# Inicializando uma lista para armazenar os resultados
results_list <- list()

# Loop sobre cada fold
for (i in seq_along(folds$splits)) {
  
  # Extraindo o split atual
  split <- folds$splits[[i]]
  
  # Obtendo os dados de treinamento e validação para este fold
  train_data <- analysis(split)
  valid_data <- assessment(split)
  
  # Treinando o modelo no conjunto de treinamento com probability = TRUE
  rd_forest_model <- ranger(
    dependent.variable.name = "operates_within_2_years",
    data = train_data,
    num.trees = 500,
    mtry = floor(sqrt(ncol(train_data) - 1)),
    importance = 'impurity',
    seed = 123,
    probability = TRUE  # Importante para obter probabilidades
  )
  
  # Fazendo previsões no conjunto de validação
  predictions <- predict(rd_forest_model, data = valid_data)
  
  # Verificando se predictions$predictions é uma matriz
  if (is.matrix(predictions$predictions)) {
    # Acessando a coluna correspondente à classe "1"
    y_pred_prob <- predictions$predictions[, "1"]
  } else {
    # Caso contrário, acessamos a probabilidade diretamente
    y_pred_prob <- predictions$predictions
  }
  
  y_pred <- as.factor(ifelse(y_pred_prob > sarrafo, 1, 0))
  
  y_real <- as.factor(valid_data$operates_within_2_years)
  
  # Armazenando os resultados em um tibble
  resultados <- tibble(
    truth = y_real,       # Valores reais
    estimate = y_pred,    # Valores previstos
    .pred_1 = y_pred_prob # Probabilidades previstas para a classe '1'
  )
  
  results_list[[i]] <- resultados
}

# Combinando os resultados de todos os folds
resultados_cv <- bind_rows(results_list)

# Atualizando a tabela models_tibble com os resultados da validação cruzada
models_tibble <- adiciona_tabela("Rd.Forest", resultados_cv, models_tibble)

roc_forest <- calcular_roc(resultados$truth, resultados$.pred_1, "Floresta Aleatória")
roc_list[[5]] <- roc_forest
```

```{r}
vip(rd_forest_model)
```

Floresta aleatoria com uso de bibliotecas:
```{r}
# Carregando as bibliotecas necessárias
library(caret)

# Definindo o seed para reprodutibilidade
set.seed(123)

# i) Preparação dos dados ------------------------------------------

# Convertendo a variável resposta para fator com níveis "no" e "yes"
treinamento$operates_within_2_years <- factor(treinamento$operates_within_2_years, levels = c(0, 1), labels = c("no", "yes"))

# ii) Definição dos parâmetros de controle para validação cruzada --

# Configurando a validação cruzada
train_control <- trainControl(
  method = "cv",                   # Validação cruzada
  number = 5,                      # Número de folds
  classProbs = TRUE,               # Computar probabilidades de classe
  summaryFunction = twoClassSummary, # Função para calcular métricas binárias
  savePredictions = TRUE           # Salvar predições para análise posterior
)

# iii) Treinamento do modelo ---------------------------------------

# Treinando o modelo de floresta aleatória usando o 'caret' e 'ranger'
rf_model <- train(
  operates_within_2_years ~ .,     
  data = treinamento,
  method = "ranger",               
  metric = "ROC",                  
  trControl = train_control,     
  tuneLength = 5                   
)

# iv) Resultados da validação cruzada ------------------------------

# Imprimindo o resumo do modelo
print(rf_model)

# Imprimindo as métricas de desempenho
print(rf_model$results)

# v) Avaliação do modelo no conjunto de treinamento ----------------

# Extraindo as predições armazenadas durante a validação cruzada
predictions <- rf_model$pred

# Calculando a curva ROC
roc_obj <- roc(response = predictions$obs, predictor = predictions$yes, levels = c("no", "yes"))

# Plotando a curva ROC
plot(roc_obj, main = "Curva ROC - Floresta Aleatória (Validação Cruzada)")

# Calculando e imprimindo a AUC
auc_value <- auc(roc_obj)
cat("AUC:", auc_value, "\n")

# vi) Métricas adicionais ------------------------------------------

# Calculando outras métricas de desempenho
conf_matrix <- confusionMatrix(predictions$pred, predictions$obs, positive = "yes")
print(conf_matrix)

# Extraindo métricas específicas
accuracy <- conf_matrix$overall['Accuracy']
precision <- conf_matrix$byClass['Precision']
recall <- conf_matrix$byClass['Recall']
f1 <- conf_matrix$byClass['F1']

# Imprimindo as métricas
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
cat("Recall:", round(recall, 4), "\n")
cat("F1 Score:", round(f1, 4), "\n")
cat("AUC:", round(auc_value, 4), "\n")

```
Mostrando a tabela com os resultados:
```{r}
models_tibble
```


Plotando todas as curvas ROC:
```{r}
# Combinando todas as curvas ROC em um único data frame
roc_data <- do.call(rbind, roc_list)

# Plotando as curvas ROC
ggplot(roc_data, aes(x = fpr, y = tpr, color = modelo)) +
  geom_line(size = 1) +
  geom_abline(linetype = "dashed") +  # Linha diagonal para referência
  labs(x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)", 
       title = "Curvas ROC de Todos os Modelos") +
  theme_minimal()
```